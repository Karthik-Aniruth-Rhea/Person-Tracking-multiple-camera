{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from sklearn.metrics import confusion_matrix , classification_report \n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = r\"D:\\CVHackathon\\archive\\train\"\n",
    "test_dir = r\"D:\\CVHackathon\\archive\\test\"\n",
    "\n",
    "SEED = 12\n",
    "IMG_HEIGHT = 48\n",
    "IMG_WIDTH = 48\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 30\n",
    "FINE_TUNING_EPOCHS = 20\n",
    "LR = 0.01\n",
    "NUM_CLASSES = 7\n",
    "EARLY_STOPPING_CRITERIA=3\n",
    "CLASS_LABELS  = ['Anger', 'Disgust', 'Fear', 'Happy', 'Neutral', 'Sadness', \"Surprise\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22968 images belonging to 7 classes.\n",
      "Found 5741 images belonging to 7 classes.\n",
      "Found 7178 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "preprocess_fun = tf.keras.applications.densenet.preprocess_input\n",
    "\n",
    "train_datagen = ImageDataGenerator(horizontal_flip=True,\n",
    "                                   width_shift_range=0.1,\n",
    "                                   height_shift_range=0.05,\n",
    "                                   rescale = 1./255,\n",
    "                                   validation_split = 0.2,\n",
    "                                   preprocessing_function=preprocess_fun\n",
    "                                  )\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                  validation_split = 0.2,\n",
    "                                  preprocessing_function=preprocess_fun)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(directory = train_dir,\n",
    "                                                    target_size = (IMG_HEIGHT ,IMG_WIDTH),\n",
    "                                                    batch_size = BATCH_SIZE,\n",
    "                                                    shuffle  = True , \n",
    "                                                    color_mode = \"rgb\",\n",
    "                                                    class_mode = \"categorical\",\n",
    "                                                    subset = \"training\",\n",
    "                                                    seed = 12\n",
    "                                                   )\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(directory = train_dir,\n",
    "                                                         target_size = (IMG_HEIGHT ,IMG_WIDTH),\n",
    "                                                         batch_size = BATCH_SIZE,\n",
    "                                                         shuffle  = True , \n",
    "                                                         color_mode = \"rgb\",\n",
    "                                                         class_mode = \"categorical\",\n",
    "                                                         subset = \"validation\",\n",
    "                                                         seed = 12\n",
    "                                                        )\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(directory = test_dir,\n",
    "                                                   target_size = (IMG_HEIGHT ,IMG_WIDTH),\n",
    "                                                    batch_size = BATCH_SIZE,\n",
    "                                                    shuffle  = False , \n",
    "                                                    color_mode = \"rgb\",\n",
    "                                                    class_mode = \"categorical\",\n",
    "                                                    seed = 12\n",
    "                                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "img_generator = img_datagen.flow_from_directory(directory = train_dir,\n",
    "                                                   target_size = (IMG_HEIGHT ,IMG_WIDTH),\n",
    "                                                    batch_size = BATCH_SIZE,\n",
    "                                                    shuffle  = True , \n",
    "                                                    color_mode = \"rgb\",\n",
    "                                                    class_mode = \"categorical\",\n",
    "                                                    seed = 12\n",
    "                                                  )\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extractor(inputs):\n",
    "    feature_extractor = tf.keras.applications.DenseNet169(input_shape=(IMG_HEIGHT,IMG_WIDTH, 3),\n",
    "                                               include_top=False,\n",
    "                                               weights=\"imagenet\")(inputs)\n",
    "    \n",
    "    return feature_extractor\n",
    "\n",
    "def classifier(inputs):\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(inputs)\n",
    "    x = tf.keras.layers.Dense(256, activation=\"relu\", kernel_regularizer = tf.keras.regularizers.l2(0.01))(x)\n",
    "    x = tf.keras.layers.Dropout(0.3)(x)\n",
    "    x = tf.keras.layers.Dense(1024, activation=\"relu\", kernel_regularizer = tf.keras.regularizers.l2(0.01))(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    x = tf.keras.layers.Dense(512, activation=\"relu\", kernel_regularizer = tf.keras.regularizers.l2(0.01))(x)\n",
    "    x = tf.keras.layers.Dropout(0.5) (x)\n",
    "    x = tf.keras.layers.Dense(NUM_CLASSES, activation=\"softmax\", name=\"classification\")(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "def final_model(inputs):\n",
    "    densenet_feature_extractor = feature_extractor(inputs)\n",
    "    classification_output = classifier(densenet_feature_extractor)\n",
    "    \n",
    "    return classification_output\n",
    "\n",
    "def define_compile_model():\n",
    "    \n",
    "    inputs = tf.keras.layers.Input(shape=(IMG_HEIGHT ,IMG_WIDTH,3))\n",
    "    classification_output = final_model(inputs) \n",
    "    model = tf.keras.Model(inputs=inputs, outputs = classification_output)\n",
    "     \n",
    "    model.compile(optimizer=tf.keras.optimizers.SGD(0.1), \n",
    "                loss='categorical_crossentropy',\n",
    "                metrics = ['accuracy'])\n",
    "  \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " input_1 (InputLayer)        [(None, 48, 48, 3)]       0         \n",
      "                                                                 \n",
      " densenet169 (Functional)    (None, 1, 1, 1664)        12642880  \n",
      "                                                                 \n",
      " global_average_pooling2d (  (None, 1664)              0         \n",
      " GlobalAveragePooling2D)                                         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               426240    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1024)              263168    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 512)               524800    \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " classification (Dense)      (None, 7)                 3591      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 13860679 (52.87 MB)\n",
      "Trainable params: 1217799 (4.65 MB)\n",
      "Non-trainable params: 12642880 (48.23 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = define_compile_model()\n",
    "clear_output()\n",
    "\n",
    "# Feezing the feature extraction layers\n",
    "model.layers[1].trainable = False\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "359/359 [==============================] - 597s 2s/step - loss: 9.1511 - accuracy: 0.3329 - val_loss: 5.3446 - val_accuracy: 0.2116\n",
      "Epoch 2/30\n",
      "359/359 [==============================] - 390s 1s/step - loss: 3.1729 - accuracy: 0.4867 - val_loss: 2.5705 - val_accuracy: 0.3588\n",
      "Epoch 3/30\n",
      "359/359 [==============================] - 387s 1s/step - loss: 1.7236 - accuracy: 0.5323 - val_loss: 1.4480 - val_accuracy: 0.5454\n",
      "Epoch 4/30\n",
      "359/359 [==============================] - 391s 1s/step - loss: 1.3484 - accuracy: 0.5577 - val_loss: 1.2516 - val_accuracy: 0.5807\n",
      "Epoch 5/30\n",
      "359/359 [==============================] - 396s 1s/step - loss: 1.2295 - accuracy: 0.5802 - val_loss: 1.4336 - val_accuracy: 0.4842\n",
      "Epoch 6/30\n",
      "359/359 [==============================] - 351s 979ms/step - loss: 1.1715 - accuracy: 0.5964 - val_loss: 1.4266 - val_accuracy: 0.5212\n",
      "Epoch 7/30\n",
      "359/359 [==============================] - ETA: 0s - loss: 1.1437 - accuracy: 0.6086Restoring model weights from the end of the best epoch: 4.\n",
      "359/359 [==============================] - 388s 1s/step - loss: 1.1437 - accuracy: 0.6086 - val_loss: 1.2667 - val_accuracy: 0.5705\n",
      "Epoch 7: early stopping\n"
     ]
    }
   ],
   "source": [
    "earlyStoppingCallback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n",
    "                                                         patience=EARLY_STOPPING_CRITERIA,\n",
    "                                                         verbose= 1 ,\n",
    "                                                         restore_best_weights=True\n",
    "                                                        )\n",
    "\n",
    "with tf.device('/gpu:0'):\n",
    "    history = model.fit(x = train_generator,\n",
    "                    epochs = EPOCHS ,\n",
    "                    validation_data = validation_generator , \n",
    "                    callbacks= [earlyStoppingCallback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = pd.DataFrame(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('emotion_epoch_30.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "359/359 [==============================] - 421s 1s/step - loss: 1.2008 - accuracy: 0.5974 - val_loss: 1.1956 - val_accuracy: 0.5924\n",
      "Epoch 2/20\n",
      "359/359 [==============================] - 464s 1s/step - loss: 1.1742 - accuracy: 0.6021 - val_loss: 1.1806 - val_accuracy: 0.5971\n",
      "Epoch 3/20\n",
      "359/359 [==============================] - 433s 1s/step - loss: 1.1593 - accuracy: 0.6127 - val_loss: 1.1720 - val_accuracy: 0.5982\n",
      "Epoch 4/20\n",
      "359/359 [==============================] - 634s 2s/step - loss: 1.1480 - accuracy: 0.6141 - val_loss: 1.1647 - val_accuracy: 0.6002\n",
      "Epoch 5/20\n",
      "359/359 [==============================] - 526s 1s/step - loss: 1.1393 - accuracy: 0.6176 - val_loss: 1.1614 - val_accuracy: 0.6011\n",
      "Epoch 6/20\n",
      "359/359 [==============================] - 454s 1s/step - loss: 1.1345 - accuracy: 0.6146 - val_loss: 1.1563 - val_accuracy: 0.6020\n",
      "Epoch 7/20\n",
      "359/359 [==============================] - 557s 2s/step - loss: 1.1303 - accuracy: 0.6210 - val_loss: 1.1522 - val_accuracy: 0.6025\n",
      "Epoch 8/20\n",
      "359/359 [==============================] - 411s 1s/step - loss: 1.1267 - accuracy: 0.6192 - val_loss: 1.1484 - val_accuracy: 0.6043\n",
      "Epoch 9/20\n",
      "359/359 [==============================] - 432s 1s/step - loss: 1.1277 - accuracy: 0.6210 - val_loss: 1.1445 - val_accuracy: 0.6051\n",
      "Epoch 10/20\n",
      "359/359 [==============================] - 531s 1s/step - loss: 1.1134 - accuracy: 0.6243 - val_loss: 1.1409 - val_accuracy: 0.6067\n",
      "Epoch 11/20\n",
      "359/359 [==============================] - 487s 1s/step - loss: 1.1124 - accuracy: 0.6270 - val_loss: 1.1374 - val_accuracy: 0.6083\n",
      "Epoch 12/20\n",
      "359/359 [==============================] - 465s 1s/step - loss: 1.1047 - accuracy: 0.6290 - val_loss: 1.1349 - val_accuracy: 0.6070\n",
      "Epoch 13/20\n",
      "359/359 [==============================] - 506s 1s/step - loss: 1.1024 - accuracy: 0.6269 - val_loss: 1.1319 - val_accuracy: 0.6065\n",
      "Epoch 14/20\n",
      "359/359 [==============================] - 841s 2s/step - loss: 1.0970 - accuracy: 0.6289 - val_loss: 1.1285 - val_accuracy: 0.6069\n",
      "Epoch 15/20\n",
      "359/359 [==============================] - 953s 3s/step - loss: 1.0903 - accuracy: 0.6356 - val_loss: 1.1268 - val_accuracy: 0.6083\n",
      "Epoch 16/20\n",
      "359/359 [==============================] - 661s 2s/step - loss: 1.0824 - accuracy: 0.6326 - val_loss: 1.1250 - val_accuracy: 0.6109\n",
      "Epoch 17/20\n",
      "359/359 [==============================] - 369s 1s/step - loss: 1.0832 - accuracy: 0.6332 - val_loss: 1.1221 - val_accuracy: 0.6137\n",
      "Epoch 18/20\n",
      "359/359 [==============================] - 365s 1s/step - loss: 1.0787 - accuracy: 0.6341 - val_loss: 1.1184 - val_accuracy: 0.6126\n",
      "Epoch 19/20\n",
      "359/359 [==============================] - 386s 1s/step - loss: 1.0810 - accuracy: 0.6324 - val_loss: 1.1169 - val_accuracy: 0.6147\n",
      "Epoch 20/20\n",
      "359/359 [==============================] - 399s 1s/step - loss: 1.0760 - accuracy: 0.6361 - val_loss: 1.1145 - val_accuracy: 0.6144\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'History' object has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mUntitled-1.ipynb Cell 10\u001b[0m line \u001b[0;36m9\n\u001b[0;32m      <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#X12sdW50aXRsZWQ%3D?line=3'>4</a>\u001b[0m     model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39moptimizers\u001b[39m.\u001b[39mSGD(\u001b[39m0.001\u001b[39m), \u001b[39m#lower learning rate\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#X12sdW50aXRsZWQ%3D?line=4'>5</a>\u001b[0m                 loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcategorical_crossentropy\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#X12sdW50aXRsZWQ%3D?line=5'>6</a>\u001b[0m                 metrics \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m      <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#X12sdW50aXRsZWQ%3D?line=7'>8</a>\u001b[0m history_ \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mfit(x \u001b[39m=\u001b[39m train_generator,epochs \u001b[39m=\u001b[39m FINE_TUNING_EPOCHS ,validation_data \u001b[39m=\u001b[39m validation_generator)\n\u001b[1;32m----> <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#X12sdW50aXRsZWQ%3D?line=8'>9</a>\u001b[0m history \u001b[39m=\u001b[39m history\u001b[39m.\u001b[39;49mappend(pd\u001b[39m.\u001b[39mDataFrame(history_\u001b[39m.\u001b[39mhistory) , ignore_index\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'History' object has no attribute 'append'"
     ]
    }
   ],
   "source": [
    "model.layers[1].trainable = True\n",
    "\n",
    "with tf.device('/gpu:0'):\n",
    "    model.compile(optimizer=tf.keras.optimizers.SGD(0.001), #lower learning rate\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics = ['accuracy'])\n",
    "\n",
    "history_ = model.fit(x = train_generator,epochs = FINE_TUNING_EPOCHS ,validation_data = validation_generator)\n",
    "history = history.append(pd.DataFrame(history_.history) , ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\CVHackathon\\object-tracking-yolov8-deep-sort\\Emoiton_Detection.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/CVHackathon/object-tracking-yolov8-deep-sort/Emoiton_Detection.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model\u001b[39m.\u001b[39msave(\u001b[39m'\u001b[39m\u001b[39memotion_epoch_fine_tuned.h5\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.save('emotion_epoch_fine_tuned.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.engine.functional.Functional at 0x26bc0505a30>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(r\"D:\\CVHackathon\\emotion_epoch_fine_tuned.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 \n",
    "img = cv2.imread(r\"D:\\CVHackathon\\archive\\test\\angry\\im7.png\")\n",
    "out = model(np.expand_dims(img , axis=0 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 7), dtype=float32, numpy=array([[0., 0., 0., 0., 1., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1f7c77c9670>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 7), dtype=float32, numpy=array([[0., 0., 0., 0., 1., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 48, 48, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.expand_dims(img , axis=0 ).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 \n",
    "\n",
    "cap = cv2.VideoCapture(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0. 0. 0. 0. 1. 0. 0.]], shape=(1, 7), dtype=float32)\n",
      "tf.Tensor([[0. 0. 0. 0. 1. 0. 0.]], shape=(1, 7), dtype=float32)\n",
      "tf.Tensor([[0. 0. 0. 0. 1. 0. 0.]], shape=(1, 7), dtype=float32)\n",
      "tf.Tensor([[0. 0. 0. 0. 1. 0. 0.]], shape=(1, 7), dtype=float32)\n",
      "tf.Tensor([[0. 0. 0. 0. 1. 0. 0.]], shape=(1, 7), dtype=float32)\n",
      "tf.Tensor([[0. 0. 0. 0. 1. 0. 0.]], shape=(1, 7), dtype=float32)\n",
      "tf.Tensor([[0. 0. 0. 0. 1. 0. 0.]], shape=(1, 7), dtype=float32)\n",
      "tf.Tensor([[0. 0. 0. 0. 1. 0. 0.]], shape=(1, 7), dtype=float32)\n",
      "tf.Tensor([[0. 0. 0. 0. 1. 0. 0.]], shape=(1, 7), dtype=float32)\n",
      "tf.Tensor([[0. 0. 0. 0. 1. 0. 0.]], shape=(1, 7), dtype=float32)\n",
      "tf.Tensor([[0. 0. 0. 0. 1. 0. 0.]], shape=(1, 7), dtype=float32)\n",
      "tf.Tensor([[0. 0. 0. 0. 1. 0. 0.]], shape=(1, 7), dtype=float32)\n",
      "tf.Tensor([[0. 0. 0. 0. 1. 0. 0.]], shape=(1, 7), dtype=float32)\n",
      "tf.Tensor([[0. 0. 0. 0. 1. 0. 0.]], shape=(1, 7), dtype=float32)\n",
      "tf.Tensor([[0. 0. 0. 0. 1. 0. 0.]], shape=(1, 7), dtype=float32)\n",
      "tf.Tensor([[0. 0. 0. 0. 1. 0. 0.]], shape=(1, 7), dtype=float32)\n",
      "tf.Tensor([[0. 0. 0. 0. 1. 0. 0.]], shape=(1, 7), dtype=float32)\n",
      "tf.Tensor([[0. 0. 0. 0. 1. 0. 0.]], shape=(1, 7), dtype=float32)\n",
      "tf.Tensor([[0. 0. 0. 0. 1. 0. 0.]], shape=(1, 7), dtype=float32)\n",
      "tf.Tensor([[0. 0. 0. 0. 1. 0. 0.]], shape=(1, 7), dtype=float32)\n",
      "tf.Tensor([[0. 0. 0. 0. 1. 0. 0.]], shape=(1, 7), dtype=float32)\n",
      "tf.Tensor([[0. 0. 0. 0. 1. 0. 0.]], shape=(1, 7), dtype=float32)\n",
      "tf.Tensor([[0. 0. 0. 0. 1. 0. 0.]], shape=(1, 7), dtype=float32)\n",
      "tf.Tensor([[0. 0. 0. 0. 1. 0. 0.]], shape=(1, 7), dtype=float32)\n",
      "tf.Tensor([[0. 0. 0. 0. 1. 0. 0.]], shape=(1, 7), dtype=float32)\n",
      "tf.Tensor([[0. 0. 0. 0. 1. 0. 0.]], shape=(1, 7), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "\n",
    "    ret , frame = cap.read()\n",
    "\n",
    "    img = cv2.resize(frame, (48, 48))\n",
    "\n",
    "    img = img.reshape(1 , 48 , 48 , 3)\n",
    "\n",
    "\n",
    "    pred = model(img)\n",
    "\n",
    "    print(pred)\n",
    "\n",
    "    #frame = np.expand_dims(frame , axis = 0)\n",
    "\n",
    "\n",
    "    org = (50, 50)\n",
    "\n",
    "    image = cv2.putText(frame, str(pred), org,  cv2.FONT_HERSHEY_SIMPLEX , 3 , (255 , 0 , 0), 2, cv2.LINE_AA)\n",
    "\n",
    "    cv2.imshow('frame' , image)\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "proglint",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
